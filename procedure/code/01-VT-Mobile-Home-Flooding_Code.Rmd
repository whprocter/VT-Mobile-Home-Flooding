---
title: "Analysis"
author: "William Procter"
date: "`r Sys.Date()`"
output: pdf_document
editor_options:
  markdown:
    wrap: sentence
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../docs/report") })
---


# Abstract

Accurate assessment of risk is an essential for effective response to any natural disaster. The methodologies
used to assess risk can end up underestimating vulnerabilities. Tropical Storm Irene offers an example of
inadequate assessment of risk, which then leads to inadequate planning for and response to a disaster. The
storm inundated Vermont with unprecedented rainfall on August 28 and 29 of 2011. The storm destroyed
480 bridges and 960 culverts (where streams cross under a road), causing $350 million in road damage and
cutting off road access to 13 mountain communities. Even Vermont's emergency management offices were
flooded! Some of the most affected people were living in mobile homes, whether on individual parcels of
land or in mobile home parks. At least 130 mobile homes were destroyed and an additional 300 severely
damaged (Figure 1). Our problem will evaluate assessments of flooding risks with a focus on mobile homes
in Vermont.
There are two different ways of assessing flooding risk in Vermont: one is by the federal agency, FEMA
(The Federal Emergency Management Association), and one by a state agency, Vermont Rivers Program.
The federal agency, FEMA, estimates flood risk in terms of inundation from rising water levels in stable
river channels. Based on existing channels, FEMA hydrologists estimate the region of land that would be
potentially flooded by a 1% (100-year) flood. The residents with mortgages in that region are required to
purchase flood insurance.
The state of Vermontâ€™s River Corridors Program estimates flooding risk differently, using river corridors.
After Irene, the state of Vermont recognized that the most damaging flooding in Vermont is not due to
inundation but rather due to fluvial erosion: the erosion of riverbanks as the river channel widens or
migrates to form new channels (Figure 1 and Figure 2). By this estimation, regions where rivers may erode
and migrate to in the future are also at risk of flooding.

The research involved a spatial overlay analysis using existing and
newly created data sets to assess the vulnerability of mobile home
parks to flooding (Fig. 3). The analysis used a number of publicly
available data sets, including Vermont E911, a mobile home park
registry, FIRMs, and orthographic imagery.


This is a reproduction of a study that is taught in Middlebury College's Human Geography with GIS course, which itself builds on Baker et al's 2011 study on **Rapid Flood Exposure Assessment of Vermont Mobile Home Parks Following Tropical Storm Irene**

Write a brief abstract about your research project.

If the project is a reproduction or replication study, include a declaration of the study type with a full reference to the original study.
For example:

This study is a *replication* of:

> citation to prior study

A graphical abstract of the study could also be included as an image here.

# Study metadata

- `Key words`: Comma-separated list of keywords (tags) for searchability. Geographers often use one or two keywords each for: theory, geographic context, and methods.
- `Subject`: select from the [BePress Taxonomy](http://digitalcommons.bepress.com/cgi/viewcontent.cgi?article=1008&context=reference)
- `Date created`: date when project was started
- `Date modified`: date of most recent revision
- `Spatial Coverage`: Specify the geographic extent of your study. This may be a place name and link to a feature in a gazetteer like GeoNames or OpenStreetMap, or a well known text (WKT) representation of a bounding box.
- `Spatial Resolution`: Specify the spatial resolution as a scale factor, description of the level of detail of each unit of observation (including administrative level of administrative areas), and/or or distance of a raster GRID size
- `Spatial Reference System`: Specify the geographic or projected coordinate system for the study, e.g. EPSG:4326
- `Temporal Coverage`: Specify the temporal extent of your study---i.e. the range of time represented by the data observations.
- `Temporal Resolution`: Specify the temporal resolution of your study---i.e. the duration of time for which each observation represents or the revisit period for repeated observations
- `Funding Name`: name of funding for the project
- `Funding Title`: title of project grant
- `Award info URI`: web address for award information
- `Award number`: award number

## Original study spatio-temporal metadata

- `Spatial Coverage`: extent of original study
- `Spatial Resolution`: resolution of original study
- `Spatial Reference System`: spatial reference system of original study
- `Temporal Coverage`: temporal extent of original study
- `Temporal Resolution`: temporal resolution of original study

# Study design

Describe how the study relates to prior literature, **reproduction study**, **reanalysis study**, or **replication study**?

Also describe the original study archetype, e.g. is it **observational**, **experimental**, **quasi-experimental**, or **exploratory**?  Original study is observational

Enumerate specific **hypotheses** to be tested or **research questions** to be investigated here, and specify the type of method, statistical test or model to be used on the hypothesis or question.

# Materials and procedure

## Computational environment

```{r environment-setup, include = FALSE}
# record all the packages you are using here
# this includes any calls to library(), require(),
# and double colons such as here::i_am()
packages <- c("downloader", "rdhs", "haven", "readr", "tidyverse", "pastecs",
             "stars", "sf", "sp", "classInt", 
             "tmap", "units", "knitr", "tibble", "here", "dplyr", "rgdal")

# force all conflicts to become errors
# if you load dplyr and use filter(), R has to guess whether you mean dplyr::filter() or stats::filter()
# the conflicted package forces you to be explicit about this
# disable at your own peril
# https://conflicted.r-lib.org/
require(conflicted)

# load and install required packages
# https://groundhogr.com/
if (!require(groundhog)) {
  install.packages("groundhog")
  require(groundhog)
}

# this date will be used to determine the versions of R and your packages
# it is best practice to keep R and its packages up to date
groundhog.day <- "2023-11-27"
set.groundhog.folder("../../data/scratch/groundhog/")

# this replaces any library() or require() calls
groundhog.library(packages, groundhog.day)
# you may need to install a correct version of R
# you may need to respond OK in the console to permit groundhog to install packages
# you may need to restart R and rerun this code to load installed packages
# In RStudio, restart r with Session -> Restart Session

# record the R processing environment
# alternatively, use devtools::session_info() for better results
writeLines(
  capture.output(sessionInfo()),
  here("procedure", "environment", paste0("r-environment-", Sys.Date(), ".txt"))
)

# save package citations
knitr::write_bib(c(packages, "base"), file = here("software.bib"))

# set up default knitr parameters
# https://yihui.org/knitr/options/
knitr::opts_chunk$set(
  echo = FALSE, # Run code, show outputs (don't show code)
  fig.retina = 4,
  fig.width = 8,
  fig.path = paste0(here("results", "figures"), "/")
)
```


## Set up file path shortcuts
```{r}
# these values allow you to access private and public raw data more efficiently
private_r <- here("data", "raw", "private")
public_r <- here("data", "raw", "public")
public_d <- here("data", "derived", "public")
scratch <- here("data", "scratch")
```



## Data and variables

There are four layers for this analysis, each coming from a different primary source.  Primary data sources for the study are to include ...

1. e911pts.shp - point - epsg: 32145
e911 point location data for all residences and buildings in Vermont, for use with emergency response.  The data file can be found on the Vermont Open GeoData Portal <http://geodata.vermont.gov/>
- SITETYPE: type of building structure/use case of the structure.  "MOBILEHOME" is the SITETYPE that indicates a site is a mobile home/

2. FEMA_100yr.shp - polygon - epsg: 32145
FEMA Flood Zone polygons with codes.  Codes starting with "A" indicate a 100-year flood risk zone. The data file can be found on the Vermont Open GeoData Portal <http://geodata.vermont.gov/>
- FLD_Zone: contains FEMA Flood Zone Codes.  If a polygon has a code begins with an "A", then that polygon indicates a 100-year flood zone.

3. river_corridors.shp - polygon - epsg: 32145
Vermont river corridor polygons, as defined by Flood Ready Vermont.  This flood hazard approach includes streams (with a 50 foot buffer) and rivers with watersheds more than 2km.  The data file can be found on the Vermont Open GeoData Portal <http://geodata.vermont.gov/>
- 

4. block_groups.shp - polygon - epsg: 32145
Census block group polygons in southern Vermont, with data on housing.  The data file was acquired from the US Census ACS Survey 2014-2018
- mobileHU: estimated total number of mobile home housing units within the block group
- totalHU: estimated total number of all housing units within the block group
- county: name of county in which the block_group is located


Each of the next subsections describes one data source.

### Primary data source1 name

- `Title`: Title of data source
- `Abstract`: Brief description of the data source
- `Spatial Coverage`: Specify the geographic extent of your study. This may be a place name and link to a feature in a gazetteer like GeoNames or OpenStreetMap, or a well known text (WKT) representation of a bounding box.
- `Spatial Resolution`: Specify the spatial resolution as a scale factor, description of the level of detail of each unit of observation (including administrative level of administrative areas), and/or or distance of a raster GRID size
- `Spatial Reference System`: Specify the geographic or projected coordinate system for the study
- `Temporal Coverage`: Specify the temporal extent of your study---i.e. the range of time represented by the data observations.
- `Temporal Resolution`: Specify the temporal resolution of your study---i.e. the duration of time for which each observation represents or the revisit period for repeated observations
- `Lineage`: Describe and/or cite data sources and/or methodological steps planned to create this data source.
  - sampling scheme, including spatial sampling
  - target sample size and method for determining sample size
  - stopping criteria for data collection and sampling (e.g. sample size, time elapsed)
  - de-identification / anonymization
  - experimental manipulation
- `Distribution`: Describe who will make the data available and how?
- `Constraints`: Legal constraints for *access* or *use* to protect *privacy* or *intellectual property rights*
- `Data Quality`: State any planned quality assessment
- `Variables`: For each variable, enter the following information. If you have two or more variables per data source, you may want to present this information in table form (shown below)
  - `Label`: variable name as used in the data or code
  - `Alias`: intuitive natural language name
  - `Definition`: Short description or definition of the variable. Include measurement units in description.
  - `Type`: data type, e.g. character string, integer, real
  - `Accuracy`: e.g. uncertainty of measurements
  - `Domain`: Expected range of Maximum and Minimum of numerical data, or codes or categories of nominal data, or reference to a standard codebook
  - `Missing Data Value(s)`: Values used to represent missing data and frequency of missing data observations
  - `Missing Data Frequency`: Frequency of missing data observations: not yet known for data to be collected

| Label | Alias | Definition | Type | Accuracy | Domain | Missing Data Value(s) | Missing Data Frequency |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| variable1 | ... | ... | ... | ... | ... | ... | ... |
| variable2 | ... | ... | ... | ... | ... | ... | ... |

### Primary data source2 name

... same form as above...




## Prior observations  

Prior experience with the study area, prior data collection, or prior observation of the data can compromise the validity of a study, e.g. through p-hacking.
Therefore, disclose any prior experience or observations at the time of study pre-registration here, with example text below:

At the time of this study pre-registration, the authors had _____ prior knowledge of the geography of the study region with regards to the ____ phenomena to be studied.
This study is related to ____ prior studies by the authors

For each primary data source, declare the extent to which authors had already engaged with the data:

- [ ] no data collection has started
- [ ] pilot test data has been collected
- [ ] data collection is in progress and data has not been observed
- [ ] data collection is in progress and __% of data has been observed
- [ ] data collection is complete and data has been observed. Explain how authors have already manipulated / explored the data.

For each secondary source, declare the extent to which authors had already engaged with the data:

- [ ] data is not available yet
- [ ] data is available, but only metadata has been observed
- [ ] metadata and descriptive statistics have been observed
- [ ] metadata and a pilot test subset or sample of the full dataset have been observed
- [ ] the full dataset has been observed. Explain how authors have already manipulated / explored the data.

If pilot test data has been collected or acquired, describe how the researchers observed and analyzed the pilot test, and the extent to which the pilot test influenced the research design.

## Bias and threats to validity

Given the research design and primary data to be collected and/or secondary data to be used, discuss common threats to validity and the approach to mitigating those threats, with an emphasis on geographic threats to validity.

These include:
  - uneven primary data collection due to geographic inaccessibility or other constraints
  - multiple hypothesis testing
  - edge or boundary effects
  - the modifiable areal unit problem
  - nonstationarity
  - spatial dependence or autocorrelation
  - temporal dependence or autocorrelation
  - spatial scale dependency
  - spatial anisotropies
  - confusion of spatial and a-spatial causation
  - ecological fallacy
  - uncertainty e.g. from spatial disaggregation, anonymization, differential privacy
  
MAUP

## Data transformations

Describe all data transformations planned to prepare data sources for analysis.
This section should explain with the fullest detail possible how to transform data from the **raw** state at the time of acquisition or observation, to the pre-processed **derived** state ready for the main analysis.
Including steps to check and mitigate sources of **bias** and **threats to validity**.
The method may anticipate **contingencies**, e.g. tests for normality and alternative decisions to make based on the results of the test.
More specifically, all the **geographic** and **variable** transformations required to prepare input data as described in the data and variables section above to match the study's spatio-temporal characteristics as described in the study metadata and study design sections.
Visual workflow diagrams may help communicate the methodology in this section.

Examples of **geographic** transformations include coordinate system transformations, aggregation, disaggregation, spatial interpolation, distance calculations, zonal statistics, etc.

Examples of **variable** transformations include standardization, normalization, constructed variables, imputation, classification, etc.

Be sure to include any steps planned to **exclude** observations with *missing* or *outlier* data, to **group** observations by *attribute* or *geographic* criteria, or to **impute** missing data or apply spatial or temporal **interpolation**.


# Read in the layers
```{r}
e911 <- read_sf(here(public_r, "e911pts.shp"))

FEMA_100yr <- read_sf(here(public_r, "FEMA_100yr.shp"))
  
river_corridors <- read_sf(here(public_r, "river_corridors.shp"))

block_groups <- read_sf(here(public_r, "block_groups.shp"))

towns <- read_sf(here(public_r, "towns.shp"))
```


# EDA

## Visualize the FEMA flood zones and river corridors
```{r}
# FEMA flood zones
tmap_mode("plot")
tm_shape(block_groups) +
  tm_fill(col="tan", alpha=0.7, title="") +
tm_shape(FEMA_100yr) +
  tm_fill(col="blue", alpha=0.7, legend.show = TRUE, title="FEMA Flood Zones") +
  tm_add_legend(title = "Flood Zones") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = 1, asp=0.8)





# River corridors
tmap_mode("plot")
tm_shape(block_groups) +
  tm_fill(col="tan", alpha=0.7, title="") +
tm_shape(river_corridors) +
  tm_fill(col="darkblue", alpha=0.7, legend.show = TRUE, title="FEMA Flood Zones") +
  tm_add_legend(title = "River Corridors") +
tm_layout(legend.width=3, legend.text.size = .6, legend.title.size = 1, asp=0.8)

```



# Clean up the flood zone and river corridor layers into more usable formats

## Clean up the FEMA data to identify all the zones that start with "A" (aka in the 100 year flood zone)
```{r}
#Clean up the FEMA data
#Combine the flood zone types


FEMA_100yr <- FEMA_100yr %>%
  mutate(flood = case_when(FLD_ZONE == "A" ~ TRUE,
                           FLD_ZONE == "AE" ~ TRUE,
                           FLD_ZONE == "AO" ~ TRUE,
                           TRUE ~ FALSE))

table(FEMA_100yr$FLD_ZONE)
FEMA_100yr
```


## Group by flooded (all of the flood zones) and dissolve the geometry to get a multipartpart flood zone
```{r}
FEMA_multipart <- FEMA_100yr %>%
  group_by(flood) %>%
  summarize(geometry = st_union(geometry))
```

## And do the same thing for river corridors...create a multipart river corridor 
```{r}
river_corridors_multipart <- river_corridors %>%
  mutate(flood = TRUE) %>%
  group_by(flood) %>%
  summarize(geometry = st_union(geometry))
```



# Calculating table columns

## Column 1: calculate the total number of mobile homes by county from the ACS data
```{r}
MHs_by_county <- block_groups %>%
  group_by(county) %>%
  dplyr::select(county, mobileHU) %>%
  summarise(number_of_MHs = sum(mobileHU)) %>%
  st_drop_geometry()

MHs_by_county 

```



## Columns 3 and 4: mobile homes at risk

### Isolate only the points that are mobile homes from the e911 data
```{r}
e911_mh <- e911 %>%
  dplyr::filter(sitetype == "MOBILE HOME")
```


### Add county variables (from the census data) to the mobile homes points
```{r}
# Join attributes by location
mh_with_county <- st_join(e911_mh, block_groups, join = st_intersects)

# Also do this for towns
mh_with_county <- st_join(mh_with_county, towns, join = st_intersects)


# Group MHs by county and sum them for each county to get column 1...****WHEN I DID THIS THE COLUMN 1 WAS INCORRECT, INVESTIGATE THIS DEVIATION!!!
MHs_by_county <- mh_with_county %>%
  group_by(county) %>%
  summarise(count_of_mobile_homes = n())
```


### Add buffer to mobile home points to account for their structure sizes
```{r}
mh_buffer <- st_buffer(mh_with_county, dist = 18.3) #18.3 meters is the average length of a mobile home

# This does NOT dissolve the resulting geometries 
```


### Identify the mobile homes within FEMA flood zone, and group by county to get total number of MHs in the flood zone by county
```{r}
# Spatial join to the overall flood zone layer...not every MH will be within the flood zone
mh_fema <- st_join(mh_buffer, FEMA_multipart, join = st_intersects)

# Filter to just the MHs that ended up in the flood zone
mh_fema <- mh_fema %>%
  dplyr::filter(flood == TRUE)

# Group by county, counting the number of MHs at risk
mh_fema_by_county <- mh_fema %>%
  group_by(county) %>%
  count(county, name = "mobile_home_count") %>%
  st_drop_geometry()

mh_fema_by_county
```


## Now do the same thing but for river corridors
## Identify the mobile homes within river corridors, and group by county to get total number of MHs in the river corridors by county
```{r}
# Spatial join to the overall river corridor layer...not every MH will be within the river corridors
mh_rc <- st_join(mh_buffer, river_corridors_multipart, join = st_intersects)

# Filter to just the MHs that ended up in the flood zone
mh_rc <- mh_rc %>%
  dplyr::filter(flood == TRUE)

# Group by county, counting the number of MHs at risk
mh_rc_by_county <- mh_rc %>%
  group_by(county) %>%
  count(county, name = "mobile_home_count") %>%
  st_drop_geometry()

mh_rc_by_county

```



## Join all columns to get final table
```{r}
final_table <- MHs_by_county %>%
  mutate(MHs_at_risk_FEMA = mh_fema_by_county$mobile_home_count) %>%
  mutate(MHs_at_risk_River_Corridors = mh_rc_by_county$mobile_home_count) %>%
  mutate(FEMA_rate = MHs_at_risk_FEMA / count_of_mobile_homes) %>%
  mutate(RC_rate = MHs_at_risk_River_Corridors / count_of_mobile_homes)
final_table
```



# Visualize MH flood risk by town
```{r}

# Group by town, counting the number of MHs at risk
#mh_fema_by_town <- mh_fema %>%
#  mutate(rc_flood = mh_rc$flood)





### Find number of mobile homes by town
mh_towns <- st_join(towns, e911_mh, join = st_intersects)

mh_towns_grouped <- mh_towns %>%
  group_by(townName) %>%
  count(townName, name = "mobile_home_count") %>%
  rename(town = townName)



### Either FEMA or River Corridor
mh_fema_nogm <- mh_fema %>%
  st_drop_geometry()

mh_rc_nogm <- mh_rc %>%
  st_drop_geometry()

mh_flooded <- full_join(mh_fema_nogm, mh_rc_nogm, by = "OBJECTID")

mh_flooded <- mh_flooded %>%
  mutate(town = ifelse(!is.na(townName.x), townName.x, townName.y)) %>%
  select(-townName.x, -townName.y)

mh_flooded_by_town <- mh_flooded %>%
  group_by(town) %>%
  count(town, name = "at_risk_count")


# Join at risk to total homes
town_data <- full_join(mh_towns_grouped, mh_flooded_by_town, by = "town")

# Change N/As to zero
town_data <- town_data %>%
  mutate(at_risk_count = ifelse(is.na(at_risk_count), 0, at_risk_count))

# Calculate an at-risk rate
town_data <- town_data %>%
  mutate(pct_mh_at_risk = at_risk_count / mobile_home_count)




# Visualize!
tmap_mode("plot")
tm_shape(town_data) +
  tm_fill(col="pct_mh_at_risk", palette=rev(hcl.colors(4, "Purple-Blue")), style = "jenks", alpha=0.7, title="") +
  tm_text("town", size = 0.3) +
  tm_borders(lwd = 0.5, col = "black") +
tm_layout(legend.width=3, legend.text.size = .6, title.size = 1, legend.title.size = .1, asp=1.5, title = "% of Mobile Homes at Risk By Town")


```



```{r}
### Discrepancy between FEMA and RC
non_overlapping <- st_difference(FEMA_multipart, river_corridors_multipart)

tmap_mode("view")
tm_basemap("Esri.WorldImagery") +
tm_shape(non_overlapping) +
  tm_fill(pattern = "stripe", col="red", palette=rev(hcl.colors(4, "Purple-Blue")), style = "jenks", alpha=0.7, title="") +
tm_layout(legend.width=3, legend.text.size = .6, title.size = 1, legend.title.size = .1, asp=1.5, title = "Discontinuities Between Flood Hazard Identification Methods")

```

Construct validity
Small number problem if a town doesn't have a ton of mobile homes
Lineage section to specify the data sources

## Analysis

Describe the methods of analysis that will directly test the hypotheses or provide results to answer the research questions.
This section should explicitly define any spatial / statistical *models* and their *parameters*, including *grouping* criteria, *weighting* criteria, and *significance thresholds*.
Also explain any follow-up analyses or validations.

# Results

Describe how results are to be presented.

# Discussion

Describe how the results are to be interpreted *vis a vis* each hypothesis or research question.

# Integrity Statement

Include an integrity statement - The authors of this preregistration state that they completed this preregistration to the best of their knowledge and that no other preregistration exists pertaining to the same hypotheses and research.
If a prior registration *does* exist, explain the rationale for revising the registration here.

# Acknowledgements

- `Funding Name`: name of funding for the project
- `Funding Title`: title of project grant
- `Award info URI`: web address for award information
- `Award number`: award number

This report is based upon the template for Reproducible and Replicable Research in Human-Environment and Geographical Sciences, DOI:[10.17605/OSF.IO/W29MQ](https://doi.org/10.17605/OSF.IO/W29MQ)

# References
